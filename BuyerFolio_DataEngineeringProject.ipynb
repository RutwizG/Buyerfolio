{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9241e0",
   "metadata": {},
   "source": [
    "# 🏡 Buyer Folio Inc. – Real Estate Agent Matching Platform\n",
    "**End-to-End Data Engineering Pipeline using Python, AWS, and ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b61dd2",
   "metadata": {},
   "source": [
    "## 🔁 Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550033db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating API response\n",
    "api_response = [\n",
    "    {\"agent_id\": 1, \"name\": \"Alice\", \"specialty\": \"Luxury Homes\", \"rating\": 4.8, \"area\": \"NY\", \"transactions\": 120},\n",
    "    {\"agent_id\": 2, \"name\": \"Bob\", \"specialty\": \"First-time Buyers\", \"rating\": 4.5, \"area\": \"CA\", \"transactions\": 80}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "data = pd.DataFrame(api_response)\n",
    "data.to_csv(\"data/raw/agent_profiles.csv\", index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868878f",
   "metadata": {},
   "source": [
    "## 🔄 Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and transform data\n",
    "df = pd.read_csv(\"data/raw/agent_profiles.csv\")\n",
    "df['performance_score'] = df['rating'] * np.log1p(df['transactions'])\n",
    "df = pd.get_dummies(df, columns=['specialty', 'area'])\n",
    "df.to_csv(\"data/processed/agent_profiles_clean.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd488bb6",
   "metadata": {},
   "source": [
    "## 🤖 Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55613335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy target variable and model training\n",
    "df['target'] = [1, 0]  # 1 = top match, 0 = not top match\n",
    "X = df.drop(columns=['name', 'agent_id', 'target'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7c056",
   "metadata": {},
   "source": [
    "## ☁️ Load Processed Data to AWS S3 (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated upload to S3\n",
    "# Normally you'd configure AWS credentials here using boto3\n",
    "\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.upload_file('data/processed/agent_profiles_clean.csv', 'my-bucket-name', 'clean_data/agent_profiles_clean.csv')\n",
    "print(\"✅ Data prepared for S3 upload.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ff530",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "We built an end-to-end pipeline that:\n",
    "- Extracted data from simulated API\n",
    "- Cleaned and transformed agent profiles\n",
    "- Built a basic ML model to simulate matchmaking logic\n",
    "- Simulated S3 integration for cloud-based storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d75ab",
   "metadata": {},
   "source": [
    "## 📊 Power BI / Looker Dashboard Sample\n",
    "To visualize the processed data in Power BI or Looker:\n",
    "\n",
    "1. Upload the `agent_profiles_clean.csv` to a cloud storage like AWS S3 or Google Cloud Storage.\n",
    "2. Connect Power BI / Looker to the cloud storage or use a cloud warehouse like BigQuery or Redshift.\n",
    "3. Build dashboards with KPIs like:\n",
    "   - Top Rated Agents\n",
    "   - Match Success Rates\n",
    "   - Agent Coverage by Region\n",
    "   - Satisfaction Scores by Specialty\n",
    "\n",
    "**Example Power BI Queries:**\n",
    "```sql\n",
    "SELECT specialty, AVG(rating) as avg_rating\n",
    "FROM agent_profiles\n",
    "GROUP BY specialty;\n",
    "\n",
    "SELECT area, COUNT(*) as match_count\n",
    "FROM matches\n",
    "GROUP BY area;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de401f",
   "metadata": {},
   "source": [
    "## 🛫 Airflow DAG Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Airflow DAG (Python representation)\n",
    "def extract():\n",
    "    print(\"Extracting data from API...\")\n",
    "\n",
    "def transform():\n",
    "    print(\"Transforming data...\")\n",
    "\n",
    "def load():\n",
    "    print(\"Uploading data to S3...\")\n",
    "\n",
    "# DAG definition\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1\n",
    "}\n",
    "\n",
    "dag = DAG('agent_matching_pipeline', default_args=default_args, schedule_interval='@daily')\n",
    "\n",
    "t1 = PythonOperator(task_id='extract', python_callable=extract, dag=dag)\n",
    "t2 = PythonOperator(task_id='transform', python_callable=transform, dag=dag)\n",
    "t3 = PythonOperator(task_id='load', python_callable=load, dag=dag)\n",
    "\n",
    "t1 >> t2 >> t3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c6bc5f",
   "metadata": {},
   "source": [
    "## 🔄 Real-Time Pipeline Simulation with Kafka (Conceptual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd266cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for real-time streaming setup using Kafka\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Producer simulating real-time feedback\n",
    "def send_feedback():\n",
    "    producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    data = {\"agent_id\": 1, \"rating\": 5.0, \"feedback\": \"Excellent service!\"}\n",
    "    producer.send('agent_feedback', data)\n",
    "    producer.flush()\n",
    "\n",
    "# Consumer to process streaming data\n",
    "def process_feedback():\n",
    "    consumer = KafkaConsumer('agent_feedback', bootstrap_servers='localhost:9092', value_deserializer=lambda m: json.loads(m.decode('utf-8')))\n",
    "    for message in consumer:\n",
    "        print(\"Received feedback:\", message.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099121c",
   "metadata": {},
   "source": [
    "## 📘 Extended Summary\n",
    "We have successfully built a simulation of a modern data engineering solution that includes:\n",
    "- Batch ingestion and transformation\n",
    "- Basic ML integration\n",
    "- Cloud-based storage (S3)\n",
    "- Real-time streaming architecture (Kafka concept)\n",
    "- Workflow orchestration (Airflow DAG)\n",
    "- Interactive analytics through Power BI / Looker"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
